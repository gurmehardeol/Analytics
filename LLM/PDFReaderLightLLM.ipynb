{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Application Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.chunks = self.process_pdf()\n",
    "        self.index = self.create_index()\n",
    "\n",
    "    def process_pdf(self):\n",
    "        chunks = []\n",
    "        with open(self.pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                chunks.extend([text[i:i+512] for i in range(0, len(text), 512)])\n",
    "        return chunks\n",
    "\n",
    "    def create_index(self):\n",
    "        embeddings = self.embedder.encode(self.chunks)\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(np.array(embeddings).astype('float32'))\n",
    "        return index\n",
    "\n",
    "    def retrieve_relevant_chunks(self, query, k=3):\n",
    "        query_embedding = self.embedder.encode([query])\n",
    "        _, indices = self.index.search(np.array(query_embedding).astype('float32'), k)\n",
    "        return [self.chunks[i] for i in indices[0]]\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(question)\n",
    "        context = \" \".join(relevant_chunks)\n",
    "        \n",
    "        input_text = f\"Answer the following question based on the given context. If the answer is not in the context, say 'I don't have enough information to answer this question.'/n/nContext: {context}/n/nQuestion: {question}/n/nAnswer:\"\n",
    "        \n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        outputs = self.model.generate(input_ids, max_length=150, num_return_sequences=1, temperature=0.7)\n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        summary = self.summarize_result(question, answer, context)\n",
    "        return answer, summary\n",
    "\n",
    "    def summarize_result(self, question, answer, context):\n",
    "        summary_prompt = f\"Summarize the following question and answer pair, mentioning the key points from the context:/n/nQuestion: {question}/n/nAnswer: {answer}/n/nContext: {context}/n/nSummary:\"\n",
    "        \n",
    "        input_ids = self.tokenizer(summary_prompt, return_tensors=\"pt\").input_ids\n",
    "        outputs = self.model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0.7)\n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gurme\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How long is a chess game. Just give me the time.\n",
      "\n",
      "Answer: The default time is 0 minutes\n",
      "\n",
      "\n",
      "*************************************\n",
      "\n",
      "\n",
      "Does chess have a queen piece. Yes or not?\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "\n",
      "*************************************\n",
      "\n",
      "\n",
      "Does chess have joker as a piece. Yes or no?\n",
      "\n",
      "Answer: No\n",
      "\n",
      "\n",
      "*************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"C:/Users/gurme/Desktop/PDFs/LawsOfChess.pdf\"\n",
    "    rag_app = RAGApplication(pdf_path)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"Enter your question (or 'quit' to exit): \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        answer, summary = rag_app.answer_question(question)\n",
    "        print(f\"\\n{question}?\")\n",
    "        print(f\"\\nAnswer: {answer}\\n\")\n",
    "        print(f\"\\n*************************************\\n\")\n",
    "        # print(f\"Summary: {summary}/n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
